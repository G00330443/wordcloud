Scalable K-Means by Ranked Retrieval?
Andrei Broder
Google
1600 Amphitheater Parkway
Mountain View, CA 94043
broder @google.com
Lluis Garcia-Pueyo
Google
1600 Amphitheater Parkway
Mountain View, CA 94043
lgpueyo@google.com
Vanja Josifovski
Google
1600 Amphitheater Parkway
Mountain View, CA 94043
vanjaj@google.com
Sergei Vassilvitskii
Google
1600 Amphitheater Parkway
Mountain View, CA 94043
sergeiv@google.com
Srihari Venkatesan
xAd
440 North Wolfe Road
Sunnyvale, CA 94085
ABSTRACT
The k-means clustering algorithm has a long history and a
proven practical performance, however it does not scale to
clustering millions of data points into thousands of clusters
in high dimensional spaces. The main computational bottleneck
is the need to recompute the nearest centroid for every
data point at every iteration, a prohibitive cost when the
number of clusters is large. In this paper we show how to
reduce the cost of the k-means algorithm by large factors by
adapting ranked retrieval techniques. Using a combination
of heuristics, on two real life data sets the wall clock time
per iteration is reduced from 445 minutes to less than 4, and
from 705 minutes to 1.4, while the clustering quality remains
within 0.5% of the k-means quality.
The key insight is to invert the process of point-to-centroid
assignment by creating an inverted index over all the points
and then using the current centroids as queries to this index
to decide on cluster membership. In other words, rather
than each iteration consisting of ¡°points picking centroids¡±,
each iteration now consists of ¡°centroids picking points¡±.
This is much more efficient, but comes at the cost of leaving
some points unassigned to any centroid. We show experimentally
that the number of such points is low and thus
they can be separately assigned once the final centroids are
decided. To speed up the computation we sparsify the centroids
by pruning low weight features. Finally, to further reduce
the running time and the number of unassigned points,
we propose a variant of the WAND algorithm that uses the
results of the intermediate results of nearest neighbor computations
to improve performance.
?Work done while the authors were at Yahoo! Research
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
WSDM¡¯14, February 24¨C28, 2014, New York, New York, USA.
Copyright 2014 ACM 978-1-4503-2351-2/14/02 ...$15.00.
http://dx.doi.org/10.1145/2556195.2556260 .
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Clustering
Keywords
k-means;WAND
1. INTRODUCTION
The web abounds in high-dimensional ¡°big¡± data: for example,
collections such as web pages, web users, search clicks,
and on-line advertising transactions. A common way to mitigate
Bellman¡¯s infamous ¡°curse of dimensionality¡± [9] is to
cluster these items: for example, classifying users according
to their interests and demographics. Among popular
approaches to clustering, the classic k-means algorithm remains
a standard tool of practitioners despite its poor theoretical
convergence properties [4, 34]. However, when clustering
n data points into k clusters, each iteration of the
k-means method requires O(kn) distance calculations, making
it untenable for clustering scenarios requiring a partition
of millions of points each with hundreds of non-null coordinates
into thousands of clusters. While parallel programming
techniques alleviate this cost by distributing the computation
across many machines, with web scaled datasets,
even massively parallelized implementation based on Hadoop
(e.g. [16]) might take thousands of CPU-hours to converge
on current hardware.
Approaches optimizing the k-means running time generally
fall into two categories: Some assume that the data is
contained in a low dimensional space and use kd-trees and
other geometric data structures to reduce the number of
distance computations [21, 28, 29]; Others (e.g. [19, 30])
assume that the number of points per cluster is large, and
subsample the data to deal with scale. Unfortunately when
both the data dimensionality is high and the average number
of points per cluster is small, both of the approaches
above fail to provide significant speed ups. This is precisely
the situation we address in this paper: we show how to reduce
the cost of the k-means algorithm by large factors by
adapting ranked retrieval techniques. Using a combination
of heuristics, on two real-world data sets the wall clock time
per iteration is reduced from 445 minutes to less than 4, and
233
from 705 minutes to 1.4, without affecting quality or convergence
speed¡ªafter 13 phases the algorithm has converged to
within 0.5% of the k-means quality with the same number
of phases.
To reduce the cost of the k-means algorithm we use nearest
neighbor data structures, optimized for retrieving the top `
closest points1
to a query point. Specifically, we use inverted
indexes and a novel variant of the WAND ranked retrieval
algorithm [12] to speed up the nearest centroid computations
at the core of the k-means method.
The k-means algorithm uses an iterative refinement technique
where each iteration consists of two steps: the assignment
step where the points are assigned to the closest
centroid, and the update step where the centroids are recalculated
based on the last assignment. The latter phase
can be performed in a linear scan over the assigned points
and is the less expensive of the two. Thus, we focus on the
use of indexing to speed up point assignment. But what
should we index? The natural choice is to index the centroids,
and then run a query for each data point, retrieving
the closest centroid. This approach would exactly implement
the k-means algorithm, but since the centroids change
in each iteration, this method requires both rebuilding the
index and evaluating n queries at each iteration. In addition,
as the centroids have many non-zero elements, they correspond
to dense documents for which top-` nearest neighbor
retrieval algorithms are not particularly efficient.
In this work we propose the converse approach. Instead
of indexing the centroids and using the points as queries, we
propose indexing the points and using centroids as queries.
This approach has several benefits. First, we do not need
to rebuild the index between iterations since the points are
stationary and only the centroids move from one iteration
to the next. Second, the number of queries per iteration is
significantly smaller (k rather than n) since we only pose
one query per centroid.
By retrieving only the top ` points closest to each centroid,
we run the risk of leaving some points unassigned to
any centroid. We show experimentally that the number of
such points is low and thus they can be separately assigned
once the final centroids are decided. In any case, these points
are precisely the outliers in the dataset (they are far away
from all known cluster centers), thus the setting of the parameter
` allows a tradeoff between the clustering speed and
the acceptable number of outliers. Finally, in many applications,
such as web search and ad selection, the application
per se requires indexing the items of interest, thus large scale
clustering of these items can be done without requiring significant
additional infrastructure.
To speed up the computation we begin by sparsifying the
centroids by pruning low weight features: we show experimentally
that this pruning brings significant improvements
in efficiency while not changing the clustering performance
at all. We then delve into the details of the WAND algorithm
[12], and modify it to remember all of the points that
were ranked in the top ` list during the retrieval. Again,
our experimental evaluation over two datasets from realistic
1we use the term top-` instead of top-k to avoid confusion
between the number of retrieved points and the number of
clusters in k-means clustering.
practical scenarios shows that the proposed approaches are
efficient and scale well with the number of clusters k, while
having virtually no impact on the quality of the results. Indeed,
in some cases, the new approaches produce clusters of
slightly better quality than the standard k-means algorithm
possibly by reducing the confusing effect of outliers.
The techniques presented in this paper can be used in a
single-machine setting, or at individual nodes in partitioned
implementations. In both cases, the proposed method reduces
the processing time and resource use by one to two
orders of magnitude, while resulting in a negligible loss in
clustering quality.
In summary the contributions of this paper are as follows:
? We describe an implementation of the k-means algorithm
that is over 100 times faster than the standard
implementation on realistic datasets by using an inverted
index over all the points and then using the
current centroids as queries into this index to decide
on cluster membership.
? We show experimentally that selectively re-assigning
in each iteration only the subset of the points that are
¡°close enough¡± to the current centroids and sparsifying
the centroids does not materially change the quality of
the final clustering.
? For retrieval in the index above, we modify the WAND
algorithm for similarity search to remember the runnerups
in the current iteration and the radius of each
cluster in the previous one, thus making significant
efficiency gains.
2. BACKGROUND
We follow the standard vector space model. Let T =
{t1, t2, . . . , tm} be a collection of m terms, and D be a collection
of n documents over the terms, D = {d1, d2, . . . , dn}.
We treat each document d ¡Ê D as a vector lying in mdimensional
term space, and use d(i) to denote the i-th coordinate.
To judge the similarity between a pair of documents,
we use the cosine similarity metric: for two documents
d, d0 ¡Ê D:
cossim(d, d0
) = d ? d
0
kdkkd
0k
,
where ? denotes the vector dot product, and k¡¤k the vector¡¯s
length. Note that since the cossim measure is a function of
the angle between the vectors d and d
0
, it is invariant under
scaling of its inputs. For any constants ¦Á, ¦Â > 0:
cossim(d, d0
) = cossim(¦Ád, ¦Âd0
).
Thus we can assume without loss of generality that all of
the documents d ¡Ê D are normalized to have unit length,
kdk = 1.
Recall that, the cossim of any pair of documents is always
between 0 and 1, and f(d, d0
) = 1 ? cossim(d, d0
) defines a
metric: that is, it is non-negative, symmetric and satisfies
the triangle inequality.
Our goal is to find a partition of the documents in D
into k non-overlapping clusters, C = {C1, C2, . . . , Ck} each
with a representative point ci that maximizes the average
cosine similarity between a document and its closest (under
234
f) representative. We will refer to the points ci as centers
of individual clusters. Formally, we want to find a partition
C which maximizes:
¦×(D, C) = X
d¡ÊD
max
c¡ÊC
cossim(c, d).
2.1 The k-means algorithm
The k-means algorithm is a widely used clustering method
[24]. In its usual formulation, the algorithm is given a point
set X ? R
d
, and a desired number of clusters k. It returns
a set C of |C| = k cluster centers {c1, . . . , ck} that forms a
local minimum for a potential function
¦Õ(X, C) = X
x¡ÊX
min
c¡ÊC
kx ? ck
2
.
In other words, it attempts to find a set of centers that
minimizes the sum of squared distances between each point
and its nearest cluster center.
The algorithm is a local search method that minimizes ¦Õ
by repeatedly (1) assigning each point to its nearest cluster
center and (2) recomputing the best cluster centers given
the point assignment.
K-means for Cosine Similarity
We adapt the k-means algorithm to maximize the average
cosine similarity, ¦×. The assignment phase remains
identical¡ªwe assign each point to the cluster center with
the maximal cosine similarity. Next we show how to compute
the optimal center given a set of points. To preserve
the structure of the algorithm, we show below that selecting
the (normalized) mean of the points as the center, maximizes
the average similarity of a cluster. A similar observation has
previously been made by Strehl et al. [32].
Lemma 1. For a set of n vectors D = {d1, d2, . . . , dn},
the unit length vector c =
P
i
di
k
P
i
dik maximizes
X
d¡ÊD
cossim(c, d).
Proof. We want to find a unit length vector c that maximizes:
X
d¡ÊD
cossim(c, d) = X
d¡ÊD
c ? d
kckkdk
=
X
d¡ÊD
c ? d = c ?
X
d¡ÊD
d
The vector c that maximizes the dot product with vector
D¡¥ =
P
d¡ÊD d must be parallel to D¡¥, setting
c =
1
kD¡¥k
X
i
di
ensures that it is parallel to D¡¥ and is of unit length.
Given the lemma, we can conclude that the k-means method
will converge to a local optimum.
Proposition 2. Given a set of documents D = {d1, . . . , dn},
the k-means method which assigns each document to its most
similar cluster, and recomputes the cluster center as the
mean of the documents assigned to it converges to a local
maximum of the objective function:
¦×(D, C) = X
d¡ÊD
max
c¡ÊC
cossim(d, ci)
.
2.2 Ranked retrieval
Inverted indexes have become a de facto standard for evaluation
of similarity queries across many different domains
due to their very low latency and good scalability properties
[35]. In an inverted index, each term t ¡Ê T has an
associated postings list which contains all of the documents
d ¡Ê D that contain t (those with d ¡É t 6= ?). For each
such document d, the list contains an entry called a posting.
A posting is composed of the document id (DID) of
d, and other relevant information necessary to evaluate the
similarity of d to the query. In this paper we denote that
additional information with d(t), and assume that it represents
a weight for the particular term in the document. Such
weights can be derived using variety of IR models [6] and in
this work we chose to use the tf-idf weighting framework,
but other methods are equally well applicable with our approach.
The postings in each list are sorted in increasing
order of DID. Often, B-trees or skip lists are used to index
the postings lists, which facilitates searching for a particular
DID within each list [35].
Inverted indexes are optimized for retrieving most similar
documents for a query under an additive scoring model.
Formally, a query q is a subset of terms, q ? T, each with
a given weight q(t). In this manner, a query can be seen as
another document. Given a similarity function g, the score
of a document for a query, is P
t¡Êd¡Éq
g(q(t), d(t)). When
working with cosine similarity, the score is q ? d, with both
q and d normalized to be unit length.
Broder et al. [12] describe WAND, a method that allows
for fast retrieval of the top-` ranked documents for a query.
We choose WAND due to its ability to scale well with the
number of features in the query, as reported in [15]. Additionally,
this is a method where we can use information
from previous rounds to further improve the retrieval time.
A major difference in our setting is the fact that queries,
representing centroids in k-means, are dense. We tak